---
description: "
"
---
#### 摘要
  Docker 作为轻量级容器技术已经为广大用户使用，它提供了应用的打包、部署、测试、运行等整个周期。Docker Registry 提供了一个存储、分发、管理 Docker 镜像文件的服务，镜像仓库服务为运行 Docker 容器提供了基础和重要的前提，在跨国部署场景下要求镜像仓库拥有高效地上传下载能力，同时做一个公网服务需要提供高可用的保障。因此，本文设计一种没有中心化节点 P2P 网络的镜像仓库服务，新服务与原镜像仓库兼容，并高效提供下载速度和保证高可用性。

#### 介绍
  Docker 是一种面向开发和运维人员进行开发、部署和运行的容器平台，相对于 Virtual Machine 更加轻量，底层使用 Linux Namespace（UTS、IPC、PID、Network、Mount、User）和 cgroups（Control Groups）技术对进程进行虚拟化隔离和资源管控，并拥有灵活性、轻量、可扩展性、可伸缩性等特点。启动一个 Docker 容器实例是从镜像文件加载的，镜像文件包含应用所需的所有可执行文件、配置文件、运行时依赖库、环境变量等。这个镜像文件可以被加载在任何装有 Docker Engine 的机器上，越来越多的开发者和公司都将自己产品的打包称 Docker 镜像文件进行发布和销售。
  在 Docker 生态中，提供镜像文件存储、分发和管理的服务叫 Docker Registry 镜像仓库服务，这种 Docker 生态重要的组成部分。用户通过 `docker push` 命令把打包好的镜像文件发布到 Registry 仓库中，其他的用户就可以通过 `docker pull` 从仓库中获取镜像文件，并由 Docker Engine 启动 Docker 实例。Docker Registry 镜像仓库，是一种集中式存储、应用无状态、节点可扩展的公共服务。提供镜像管理和存储、上传下载、AAA 认证鉴权、WebHook 通知、日志等功能。几乎所有的用户都从镜像仓库中进行上传和下载，无疑在跨国下载的场景下，存在性能瓶颈，在网络延迟大的情况下，用户 pull 下载消耗更长的时间。同时因为是集中式服务，也容易收到黑客的 DDos 攻击。当然你可以部署多个节点，但也面临如多节点间镜像文件同步的问题。因此，需要一个去中心化的分布式镜像仓库。这篇文章起草了一个新的纯 P2P 式网络设计的无中心化节点新型的镜像仓库服务 Decentralized Docker Registry(DDR)，和阿里的蜻蜓 Dragonfly、腾讯的 FID 混合型 P2P 模式不同，DDR 采用纯 P2P 网络结构，没有镜像文件 Tracker 管理节点，网络中所有节点既是镜像的生产者同时也是消费者，这种结构能有效地防止拒绝服务 DDos 攻击，没有单点故障，拥有高水平扩展和高并发能力，并就近节点路由下载，高效利用带宽，极速提高下载速度。

#### 镜像文件
  Docker 是一个容器管理框架，它负责创建和管理容器实例，一个容器实例从 Docker 镜像文件创建而来，而镜像文件类似一种压缩文件，包含了一个应用所需的所有可执行文件、配置文件、运行时依赖文件、环境变量等。容器实例启动时，将镜像文件加载，一个镜像可以依赖另一个镜像，是一种单继承关系，最底层的镜像叫做 Base 基础镜像，所有的用户镜像都可以继承 Base 镜像制作新镜像，也可以继承其他的镜像，“其他的镜像”被叫做 Parent 父镜像。一个镜像内部又被切分称多个层级 Layer，没一个 Layer 包含整个镜像的部分文件。当 Docker 容器实例从镜像加载后，将看到所有 Layer 共同合并的文件集合，镜像里面所有的 Layer 属性为只读，当容器进行写操作的时候，就产生新的 Layer 层级，新 Layer 包含新写入或删除的文件。这种结构能最大化的节省空间，并能高效复用。一个典型的镜像结构如下：

  ![img](/images/images.png)

  alpine 是基础镜像，提供了一个轻量的、安全的 Linux 运行环境，Basic App1 和 Basic App2 都基于和共享这个基础镜像 alpine，Basci App 1/2 可作为一个单独的镜像发布，同时也是 Advanced App 2/3 的父镜像，在 Advanced App 2/3 下载的时候，会检测并下载所有的父镜像和基础镜像，而往往在 registry 存储节点里，只会有一个父镜像实例和基础镜像实例，从而高效节省存储空间。


   一个镜像内部分层 Layer 结构如下：

   ![img](/images/rw_layer.png)

   Advanced App1 内部文件分为 5 个 layer 层存储，每一层在 registry 中都为 application/vnd.docker.image.rootfs.diff.tar.gzip 压缩类型文件，每一层存储自己的文件内容，每一个 layer 都通过文件 sha256 值进行标示，所有 layer 层的文件组成了最终镜像的内容，在容器从镜像启动后，所有 layer 层的文件内容 merge 合并称一个整体的文件结构。

```sh
$ file /var/lib/registry/docker/registry/v2/blobs/sha256/40/4001a1209541c37465e524db0b9bb20744ceb319e8303ebec3259fc8317e2dec/data
data: gzip compressed data
$ sha256sum /var/lib/registry/docker/registry/v2/blobs/sha256/40/4001a1209541c37465e524db0b9bb20744ceb319e8303ebec3259fc8317e2dec/data
4001a1209541c37465e524db0b9bb20744ceb319e8303ebec3259fc8317e2dec
```
   其中实现这种分层模型的文件系统叫 UnionFS 联合文件系统，UnionFS 的实现主要有 AUFS、Overlay、Overlay2 等实现，主要实现了只读目录和可写目录，并把所有目录文件映射到另一个挂载目录下，当在文件写操作时，采用 copy-on-write 从原目录中拷贝一份文件到新目录下，然后再对此文件进行写操作，形成新的目录，UnionFS 把这种目录叫做 Branch，也就是镜像文件中的 Layer。

   例如，使用 AUFS 构建一个 2 层 Layer 如下：

   ```sh
   $ mkdir /tmp/rw /tmp/r /tmp/aufs
   $ mount -t aufs -o br=/tmp/rw:/tmp/r none /tmp/aufs
   ````
   创建了2个层级目录分别是 /tmp/rw 和 /tmp/r，同时 br= 指定了所有的 branch 层，默认情况下 br=/tmp/rw 为可写层，: 后面只读层，/tmp/aufs 为最终文件挂载层，文件目录如下：

```sh
$ ls -l /tmp/rw/
-rw-r--r-- 1 root       root       23 Mar 25 14:21 file_in_rw_dir

$ ls -l /tmp/r/
-rw-r--r-- 1 root       root            26 Mar 25 14:20 file_in_r_dir

$ ls -l /tmp/aufs/
-rw-r--r-- 1 root       root            26 Mar 25 14:20 file_in_r_dir
-rw-r--r-- 1 root       root            23 Mar 25 14:21 file_in_rw_dir
```
可以看到挂载目录 /tmp/aufs 下是 /tmp/rw 和 /tmp/r 目录下的文件集合，这种效果就是 Image 镜像文件被加载的效果。

#### Docker Registry
  Docker Registry 镜像仓库存储、分发和管理着镜像，流行的镜像仓库服务有 Docker Hub、Quary.io、Google Container Registry。每一个用户可以在仓库内注册一个 namespace 命名空间，用户可以通过 `docker push` 命令把自己的镜像上传到这个 namespace 命名空间内，其他用户可以使用 `docker pull `命令从此命名空间中下载对应的镜像，同时一个镜像名可以配置不同的 tags 用以表示不同的版本。
#### Push 上传镜像
  当要上传镜像时，Docker Client 向 docker daemon 发送 push 命令，首先创建对应的 manifest 元信息，并通过 HEAD <name>/blob/<digest> 检查需要上传的 layer 在 Registry 仓库中是否存在，如果存在则无需上传，否则通过 POST <name>/blob/upload 进行上传，docker 使用 PUT 分段并发上传，每一次上传一段文件的 bytes 内容，最终 layer 上传完成后，通过 PUT <name>/manifest/<tag> 完成整个上传过程。
#### Pull 下载镜像
  当用户执行 `docker pull` 命令时，pull 命令先传递给本地的 Docker daemon 守护进程，如果不指定 host 名字，默认 docker daemon 会从 Docker hub 官方仓库进行下载，首先向 Hub 发送 GET <namespace>/manifest/<tag> 请求，返回镜像名字、Layers 层等信息，然后通过 HEAD <name>/blob/<digest> 检查镜像内 Layer 是否存在，如果存在，通过 GET <name>/blob/<digest> 对所有 Layer 进行并发下载，默认 Docker daemon 会并发3个 layer 下载。

#### P2P 网络

  P2P 网络从中心化程度看分为纯 P2P 网络和杂 P2P 网络，纯网络没有任何中心服务器和路由器，每一个节点同时作为客户端和服务端，如 Gnutella 协议。杂 P2P 网络的节点除了客户端和服务端外，还需要中心服务器保存和维护着节点、内容等元信息，如 BitTorrent 协议。本系统使用纯 P2P 网络实现，不设定中心节点，镜像文件由整个网络组织和管理。

  ![img](/images/pure-peer.png)
  纯 P2P 网络

  ![img](/images/hybrid-peer.png)
  混合 P2P 网络

  从网络拓扑结构看分为结构化 P2P 网络和非结构 P2P 网络，Peer 节点之间彼此之间无规则随机连接生成的网状结构，称之为非结构 P2P，如 Gnutella 。而 Peer 节点间相互根据一定的规则连接，称之为结构 P2P，如 Kademlia。本系统使用结构 P2P 来连接节点，形成一个规则网络，用于文件搜索。

  ![img](/images/unstruct-peer.png)
  非结构 P2P，之间无序不规则连接

  ![img](/imgas/struct-peer.png)
  结构 P2P，按照一定的规则相互互联

  DDR 镜像仓库系统采用纯结构化 P2P 网络，每一个节点至少与一个节点建立连接，并使用 DHT（Distribution Hash Table) 的 Kademlia 规则建立结构化网路，每一个节点使用与文件 sha256 一样的 256-bit 作为节点 ID，每一个节点维护一张动态路由表，把全网的路由节点 NodeID 根据二叉树结构表示，即 256 bit 位的节点 ID 可表示最大有 256 个子树，每一个子树下包含2^i(0<=i<=256)到2^i+1(0<=i<=255)个叶子节点，如 i=2 的子树包含二进制 100、101、110、111 的4个节点，这样每一个区间成为 bucket 桶。根据二叉树的结构，知道任何一棵子树的一个节点就能递归找到任意节点。因此，当前节点对周边节点进行子树 bucket 拆分，把不同“距离”的节点存储在不同的 bucket 路由表里，如下：

![img](/images/kademlia.png)

  而计算节点之间的距离是“逻辑距离”，并不是物理距离，是节点 ID 与节点 ID 之间 XOR 异或运算的值，如 X 与 Y 的距离 dis(x,y) = NodeIDx XOR NodeIDy，XOR 异或运算符合如下3个几何特性：
    1. X 与 Y 节点的距离等于 Y 与 X 节点的距离，即 dis(x,y) = dis(y,x)，异或运算之间的距离是对称的。
    2. X 与 X 节点的距离是 0，异或运算是等同的。
    3. X、Y、Z 节点之间符合三角不等式，即 dis(x,y) <= dis(x,z) + dis(z,y)

  为了提高可用性，设定每一个子树保留最多 5 个节点路由信息，每一条路由记录包含<IP 地址、UDP 端口、NodeID>元素，并根据网络请求进行动态学习和更新，如节点收到任何消息时，如果此节点存在路由表中，则根据 LRU 规则更新这条路由，如果不存在，则新增一条路由，当路由表达到上限 5 条时，则剔除失效的节点，如无可失效的节点，则丢弃更新。


  ##### 查询节点
  当节点需要查询某个 NodeID 时，先计算目标 NodeID 在当前哪个子树区间（bucket 桶）中，并向此 bucket 桶中 n(n<=5) 节点同时发送 FIND_NODE RPC 请求，n 个节点收到 FIND_NODE 请求后根据自己的路由表信息返回与目标 NodeID 最接近的节点 NodeID，源节点根据 RPC 返回的路由信息进行学习，再次向 RPC 返回的节点发送 FIND_NODE RPC 请求，以此迭代，每一次迭代至少保证精确一个 bit 位，并最终找到目标节点，查询速度收敛到 logN，因此，Kademlia 通过不断递进的路由信息和迭代查询的方式在全网查询节点。

  ![img](/images/kad-find-node.png)

  ##### 查询镜像
  查询节点并不能解决镜像文件的问题，最终在网络中需要找到指定的镜像文件，而全网查询仅有节点 NodeID，因此常用的做法是建立节点 NodeID和文件 fileID 的映射关系，但这需要依赖全局 Tracker 节点来记录映射关系，而在纯 P2P 模式下无法做到，因此，本文使用类似一种代理的模式，这种模式下分为消费节点、代理节点、生产节点、副本节点4种角色，生产节点为镜像文件真正制作和存储的节点，当新镜像制作出来后，把镜像 Image Layer 的 sha256 fileID 作为参数进行 FIND_NODE 查询与 fileID 相近或相等的 NodeID 节点，这些节点即被当作镜像文件的 Proxy 代理节点，并存储生产节点的 IP、Port、NodeID 信息，此代理同时作为副本节点对生产节点产生的新镜像文件进行缓存工作。当消费节点下载镜像文件 Image Layer 时，同样使用 fileID 作为参数通过 FIND_NODE 查找代理节点，再向代理节点发送 FIND_VALE 请求返回真正镜像的生产节点，消费节点则直接向消费节点进行 docker pull 镜像拉取工作。如果代理节点缓存了镜像文件，则直接返回生产节点镜像内容。

  #### 镜像元信息代理和镜像 Layer 代理
  对于镜像消费者来讲只知道需要下载的镜像名:tag，并不知道镜像内的 Layer sha256 值，如 docker pull os/centos:7.2，因此，在发布和下载镜像时，首先以镜像名:tag作为输入生成随机的 sha256 值和 Layer sha256 内容一样推送给代理节点，不同的 sha256 地址将会推送到不同的代理节点中，当消费节点需要下载镜像时，先找到镜像元信息，元信息代理节点返回镜像文件 Image Layer sha256 列表，消费节点再根据 sha256 列表去镜像 Layer 代理获取生成节点或缓存节点信息，并进行并发下载。



#### DDR 架构

  ![img](/images/ddr_arch.png)
  DDR 分为 DDR Driver 和 DDR Daemon，DDR Driver 作为 Docker Registry 的存储插件承接 Registry 的 blob 和 manifest 数据的查询、下载、上传的工作，并与 DDR Daemon 交互，主要对需要查询的 blob 和 manifest 数据做 P2P 网络寻址。DDR Daemon 作为 P2P 网路中一个 Peer 节点接入，负责 Peer 查询、Blob、Manifest 的路由查询，并返回路由信息给 DDR Driver，DDR Driver 作为 Client 根据路由去 P2P 网络目的 Docker Registry 节点发送 Push/Pull 请求。

#### DDR 与 Docker Registry 集成
  docker registry 镜像仓库服务采用可扩展性的设计，允许开发者自行扩展存储驱动以实现不同的存储要求，当前仓库官方支持内存、本地文件系统、S3、Azure、swift 等多个存储，驱动需要实现如下接口(registry/storage/driver/storagedriver.go)：

  ```go
  // StorageDriver defines methods that a Storage Driver must implement for a
  // filesystem-like key/value object storage. Storage Drivers are automatically
  // registered via an internal registration mechanism, and generally created
  // via the StorageDriverFactory interface (https://godoc.org/github.com/docker/distribution/registry/storage/driver/factory).
  // Please see the aforementioned factory package for example code showing how to get an instance
  // of a StorageDriver
  type StorageDriver interface {
  	Name() string
  	GetContent(ctx context.Context, path string) ([]byte, error)
  	PutContent(ctx context.Context, path string, content []byte) error
  	Reader(ctx context.Context, path string, offset int64) (io.ReadCloser, error)
  	Writer(ctx context.Context, path string, append bool) (FileWriter, error)
  	Stat(ctx context.Context, path string) (FileInfo, error)
  	List(ctx context.Context, path string) ([]string, error)
  	Move(ctx context.Context, sourcePath string, destPath string) error
  	Delete(ctx context.Context, path string) error
  	URLFor(ctx context.Context, path string, options map[string]interface{}) (string, error)
  	Walk(ctx context.Context, path string, f WalkFn) error
  }
  ```

  DDR Driver 实现以上所有接口，在上传的时候把内容存储在本地磁盘，同时根据 sha256 的信息向 P2P 网络中的其他节点发布路由信息。


##### DDR push 上传镜像
  Docker Client 向本地 Docker Registry 上传一个镜像时会触发一系列的 HTTP 请求，这些请求会调用 `DDR Driver` 对应的接口实现，DDR 上传交互流程如下：
  1. Client 通过 HEAD /v2/hello-world/blobs/sha256:9bb5a5d4561a5511fa7f80718617e67cf2ed2e6cdcd02e31be111a8d0ac4d6b7 判断上传的 blob 数据是否存在，如果本地磁盘不存在，Registry 返回 404 错误；
  2. POST /v2/hello-world/blobs/uploads/ 开始上传的 blob 数据；
  3. PATCH /v2/hello-world/blobs/uploads/ 分段上传 blob 数据；
  4. PUT /v2/hello-world/blobs/uploads/ 完成分段上传 blob 数据，DDR 根据 blob 文件的 sha256 信息寻找 P2P 网络中与目标 sha256 值相近的 k 个节点，发送包含 blob sha256 的 STORE 消息，对端 Peer 收到 sha256 信息后，存储源 Peer 节点 IP、Port、blob sha256信息到本地 KV 数据库中；
  5. HEAD /v2/hello-world/blobs/sha256:9bb5a5d4561a5511fa7f80718617e67cf2ed2e6cdcd02e31be111a8d0ac4d6b7 确认上传的数据是否存在，Registry 返回 200 成功；
  6. PUT /v2/hello-world/manifests/latest 完成 manifest 元数据上传，DDR Driver 按照 <namespace>/manifest/<tag> 做 sha256 计算值后，寻找 P2P 网络中与目标 sha256 值相近的 k 个节点，发送包含 manifest sha256 的 STORE 消息，对端 Peer 收到 sha256 信息后，存储源 Peer 节点 IP、Port、blob sha256信息到本地 KV 数据库中；

##### pull 下载镜像
  Docker Client 向本地 Docker Registry 下载镜像时会触发一系列的 HTTP 请求，这些请求会调用`DDR Driver`对应的接口实现，DDR 下载交互流程如下：
  1. GET /v2/hello-world/manifests/latest 返回<namespace>下某个<tag> 的 manifest 源信息，DDR Driver 对 hello-world/manifest/latest 进行 sha256 计算，并向 P2P 网路中发送 FIND_NODE 和 FIND_VALUE 找到目标 sha256 的真正存储 Peer 节点，并向存储节点发送 GET 请求获取 manifest 元信息。
  2. Client 获取 manifest 元信息后，通过 GET /v2/hello-world/blobs/sha256:e38bc07ac18ee64e6d59cf2eafcdddf9cec2364dfe129fe0af75f1b0194e0c96 获取 blob 数据内容，DDR Driver 以 e38bc07ac18ee64e6d59cf2eafcdddf9cec2364dfe129fe0af75f1b0194e0c96 作为输入，向 P2P 网络中发送 FIND_NODE 和 FIND_VALUE 找到存储节点 Peer，并向存储 Peer 节点发送 GET 请求获取 blob 数据。




  http://dockone.io/article/2877
