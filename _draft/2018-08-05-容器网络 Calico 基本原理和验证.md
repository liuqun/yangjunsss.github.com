---
description: "
在容器网络跨 Host Overlay 方案中，除开 Flannel 使用的隧道方案实现，第二种主流的方案为 Calico 三层路由的方案实现，与 Flannel 不同的是 Calico 并不使用隧道或 NAT 来转发和组网，而是巧妙的把流量转换称三层路由的方式，再直接利用 host 上 veth 接口和路由表来完成数据转发，本文则对 Calico 进行基本原理分析和模拟验证。
"
---
### 摘要
在容器网络跨 Host Overlay 方案中，除开 Flannel 使用的隧道方案实现，第二种主流的方案为 Calico 三层路由的方案实现，与 Flannel 不同的是 Calico 并不使用隧道或 NAT 来转发和组网，而是巧妙的把流量转换称三层路由的方式，再直接利用 host 上 veth 接口和路由表来完成数据转发，本文则对 Calico 进行基本原理分析和模拟验证。

### 简介

calico 官方定义如下：

```text
Calico provides secure network connectivity for containers and virtual machine workloads.

Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.

Calico also provides dynamic enforcement of network security rules. Using Calico’s simple policy language, you can achieve fine-grained control over communications between containers, virtual machine workloads, and bare metal host endpoints.
```
大意翻译称自己的理解是：
Calico 为容器和 vm 提供一个安全的网路连接方案，通过给 workload 分配一个扁平的三层路由可达的 IP 地址，并他们之间不使用隧道或 NAT 技术，这种简单的方式可以提供更好的网络性能、易维护、可交互性。在一些需要 overlay 的场景种，也提供 IPIP 隧道连接和与 Flannel 集成。
Calico 同时提供了一种动态执行的网络安全策略，能使用简单的安全模型语言实现复杂的 workloads 之间的安全控制。

### 相对 Overlay，为什么用 Calico？

Calico 有一篇官方的[博文](https://www.projectcalico.org/why-calico/)来说明，大意如下：

Calico 是一种使 workloads 之间互通的网络方法，这里特意使用 workloads 来代替 VM 或 Container 等是因为 Calico 能应用在以上任意场景中。
在虚拟化平台中，像 OpenStack、Docker 等都需要实现 workloads 之间互通，但同时也需要限制 workloads 之间互通，就像仅开放部分端口被 internet 访问一样，也需要对多个租户之间做网络隔离。
而在传统的虚拟化平台中，也是现实中大多数小型系统通常都使用二层隔离技术来配置 workloads 的网络，这种方法有一些弊端：平台需要实现 VLAN、bridge 和隧道来保证二层网络在 hosts 之间互通。bridge 带来一定的复杂性，vlan 和 tunnel 隧道将消耗更多的资源并对物理环境有限制，随着网络规模增大，配置也会越复杂。
那么有更好的方案吗？在审视了二层方案并思考如何支持大型网络时从 internet 网络中获得了灵感。在 internet 的网路中，路由器连接着各种小网络，最终组成一个巨大的 internet 的网络，路由器之间通过 BGP 相互学习路由，并使用防火墙控制不同主机之间的连接，这种方式是否也能应用到虚拟网络中呢？
因此，借鉴同样思路，把 host 当作 internet 中的路由器，称之为虚拟路由器 vRouter，同样使用 BGP 同步路由，在 host 上的 iptables 来控制访问连接，最终设计了 Calico 系统，整个系统的优势为：

1. 更优的资源利用
  二层网络通讯依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，calico 完全不依赖二层广播。而且，如果你使用 VLAN 技术，将会面临 4096 个规格限制，即便你可以使用 vxlan 技术解决这个问题，但 vxlan 又带来了新的隧道开销的问题。
2. 可扩展性
  calico 使用与 internet 类似的方案，internet 的网络比任何数据中心都大，calico 同样也具有可扩展性。
3. 简单而容易 debug
  因为没有隧道，意味着 workloads 之间更简单，更少的配置，更容易在 host 上进行 debug。
4. 对数据中心更少的依赖
  Calico 仅依赖三层路由，不依赖 L2 层的技术栈。
5. 可适配性
  Calico 不依赖任何虚拟化模型，所以它能适配所有 VM、Container、白盒或者混合环境。

除了以上，还有更多的优势，如果你为 OpenStack 或 docker 构建虚拟化网络环境的话，好好考虑下 calico 的方案。

### 架构

Calico 由 5 部分组件组成，整体构架如下：
![img](http://yangjunsss.github.io/images/calico/calico_arch.png)

1. Felix，运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等
2. Orchestrator Plugin，编排插件，并不是独立运行的某些进程，而是设计与 k8s、OpenStack等平台集成的插件，如 Neutron’s ML2 plugin 用于用户使用 Neutron API 来管理 Calico，本质是要解决模型和 API 间的兼容性问题。
3. Etcd，Calico 模型的存储引擎。
4. BGP Client（BIRD），Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉其他与它互联的 Host 节点，如某个 IP 在我这，下一跳指向我，从而实现网络互通。
5. BGP Route Reflector(BIRD)，在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个问题，可以采用 BGP 的 Router Reflector 的方法，简单说所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。


### 模拟组网

组网如下：

![img](http://yangjunsss.github.io/images/calico/calico_network.png)

1. guest 配置 169.254.1.1 的默认路由
2. host 上配置 10.20.2.0/24 和 10.20.1.3/32 路由
3. 开启 arp proxy 和 ip_forward 能力

网络连通性测试：

```sh
# HOST0
[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.2.2
PING 10.20.2.2 (10.20.2.2) 56(84) bytes of data.
64 bytes from 10.20.2.2: icmp_seq=1 ttl=62 time=0.774 ms
64 bytes from 10.20.2.2: icmp_seq=2 ttl=62 time=0.332 ms
[root@i-7dlclo08 ~]# ip netns exec ns0 ping 192.168.100.3
PING 192.168.100.3 (192.168.100.3) 56(84) bytes of data.
64 bytes from 192.168.100.3: icmp_seq=1 ttl=63 time=1.00 ms
64 bytes from 192.168.100.3: icmp_seq=2 ttl=63 time=0.695 ms
[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.1.3
PING 10.20.1.3 (10.20.1.3) 56(84) bytes of data.
64 bytes from 10.20.1.3: icmp_seq=1 ttl=62 time=957 ms
64 bytes from 10.20.1.3: icmp_seq=2 ttl=62 time=0.432 ms
64 bytes from 10.20.1.3: icmp_seq=3 ttl=62 time=0.563 ms
```

转发过程：
1. guest0 本地所有数据包都转发到一个虚假的地址 169.254.1.1，发送 ARP Req。
2. Host0 的 veth 端收到 ARP Req 时通过开启网卡的 `/proc/sys/net/ipv4/conf/veth0/proxy_arp` ARP 代理功能直接把自己的 MAC 地址返回给 guest0
3. guest0 发送目的地址为 guest1 的数据包
4. Host 查询本地路由 `10.20.2.0/24 via 192.168.0.3 dev eth0 proto bird` 发送给对端 host1，看到 proto 协议为 BIRD 实现，这里为空
5. 在发送之前查询匹配本地的 iptables 规则进行安全检查
6. 当对方收到送往 10.20.2.2 的数据包时查找本地路由表匹配`10.20.2.2/32 dev veth0 scope link` 转发到对应的 veth 端从而到达 guest1

### BGP 模型
在整个 calico 的方案中，因为使用 BGP 来同步路由，那么 BGP 模型设计成为一个重要话题，calico 官方 propose 了3种模型，并推荐 AS Per Rack model：
1. AS Per Rack model
2. AS Per Computer Server model
3. Downward Default model

```text
Recommendation
The Project Calico team recommends the use of the AS per rack model if the resultant routing table size can be accommodated by the ToR and spine switches, remembering to account for projected growth.

If there is concern about the route table size in the ToR switches, the team recommends the Downward Default model.

If there are concerns about both the spine and ToR switch route table capacity, or there is a desire to run a very simple L2 fabric to connect the Calico nodes, then the user should consider the Ethernet fabric as detailed in this post.
```

### 疑问和思考
1. 租户隔离问题
    calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。
2. 路由规模问题
    通过路由规则可以看出，路由规模和 guest 分布有关，如果 guest 离散分布在 host 上，势必会产生较多的路由项。


### 总结
1. calico 通过巧妙的引导 workload 所有的流量 169.254.1.1 这个不存在的网关，并引流到 host 的 calixxx，形成了二三层流量全部转换称 host 的三层转发来实现流量转发
2. 在 Host 上通过开启 arp proxy 的能力实现 arp 代答，arp 广播被限制在了 host 上，并且 arp 记录都“无效”，没有广播风暴和 arp 表膨胀的问题
3. 使用 iptables 在 host 做 policy 实现的复杂的安全模型，虽然有一个中心配置节点，但策略应用在每一台虚拟路由器上，形成了一个分布式的安全模型
