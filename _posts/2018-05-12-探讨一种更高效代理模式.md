---
description: "
Service Mesh 提供了微服务化开发的新思路，核心思想主要是构建一个代理转发网络结合控制和转发分离的做法来对千百个微服务做流量、策略、安全等管理，核心分为控制和数据两大组件，而另一方面 Linux Kernel 提供一种运行时高效扩可编程的网络注入机制 eBPF，并借此能实现 L47 层代理转发。假如借助 eBPF，并应用到 Service Mesh 的数据转发层，作为一个新组件，对接 Pilot、Mixer 控制面，实现控制层的策略和流量管理，是否是一种高效的方式？这种方式比使用 Envoy 具有更好的性能，虽然性能未必是 Mesh 首要考虑的问题，倒不妨是一种有趣的新尝试。
"
---

#### 背景和想法

Service Mesh 提供了微服务化开发的新思路，核心思想主要是构建一个代理转发网络结合控制和转发分离的做法来对千百个微服务做流量、策略、安全等管理，核心分为控制和数据两大组件，而另一方面 Linux Kernel 提供一种运行时高效扩可编程的网络注入机制 eBPF，并借此能实现 L47 层代理转发。假如借助 eBPF，并应用到 Service Mesh 的数据转发层，作为一个新组件，对接 Pilot、Mixer 控制面，实现控制层的策略和流量管理，是否是一种高效的方式？这种方式比使用 Envoy 具有更好的性能，虽然性能未必是 Mesh 首要考虑的问题，倒不妨是一种有趣的新尝试。（后发现 Cilium 果然已经对接 Istio 了，具体详情 [http://docs.cilium.io/en/latest/gettingstarted/istio/](http://docs.cilium.io/en/latest/gettingstarted/istio/)），在 Istio 看来 Envoy 作为 Pod 里的一个 SideCar 容器运行，而 Cilium 把 Envoy 运行在应用 Pod 之外，为每一个 Pod 运行一个单独的监听器，这两种模型各有利弊，没有孰好孰坏。
Istio 集成 Cilium 架构如下：

![img](/images/istio_overview-7e6d28cafd5bf0e1e276b7ff7a72ab35-84ad3.png)

同时 Cilium 做了一个延迟性能对比，Pod - Proxy - Pod 下的代理性能如下：

![img](/images/proxy_latency-cfa84e044543201194706ffead9a024c-84ad3.png))

#### Service Mesh

2016 年，两个不为人知的小项目 linked 和 Envoy 肯定没有想到两年后的今天，火遍了微服务圈，开启了一个全新的微服务实现思路：无侵入式微服务架构，这种架构有星星燎原之势取代旧服务框架 SpringCloud，新思路解耦了微服务管理和应用实现，应用层面不再关心服务注册、发现、策略等等，而只需要把请求发送给代理服务器，也就是新架构下的 SideBar，应用可以用任何语言实现，对外的接口纯粹编程了协议交互，没有代码级别的耦合，我被这种优雅的架构所感动。认为这种架构更具有灵活性。在一个大型的应用系统中，模块的复杂度是常人难以想象的，甚至会让开发和架构师感到疲惫不堪和力不从心，而框架的职责是把复杂度下沉，下沉的方式通常有几个力度，比如下层到框架代码层面，下沉到服务进程层面，Mesh 的思想属于后者，这使得应用本身变得轻量，而且没有框架依赖，框架也不会有语言依赖。甚至我们大胆想象下，数据库的访问是否也可以从框架代码层面下沉到服务进程层面，即应用访问数据不再依赖 JDBC 这种 SDK，而以一种更加通用的协议交互。

SpringCloud 是一种服务化应用框架，帮助应用快速进行微服务化开发，官方解释如下：

`Spring Cloud provides tools for developers to quickly build some of the common patterns in distributed systems (e.g. configuration management, service discovery, circuit breakers, intelligent routing, micro-proxy, control bus, one-time tokens, global locks, leadership election, distributed sessions, cluster state). Coordination of distributed systems leads to boiler plate patterns, and using Spring Cloud developers can quickly stand up services and applications that implement those patterns.`

提供了配置管理、服务发现、服务环路发现、智能路由、代理、全局锁、集群选主、分布式 Session、集群状态管理等集群服务下所需要的基本能力。

2016年9月，Linked 背后的创业公司 Buoyant 第一次在 SF Microservices 提出了一个新概念“Service Mesh”，并随后在 2017 年 4月 Buoyant 的 William Morgan 给 Service Mesh 做了定义：

`A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.`

**are deployed alongside application code, without the application needing to be aware** 这句话就是 mesh 的核心，换句话说，使用一种应用无感知和无侵入的方式实现集群服务。微服务框架从应用层面剥离出来，从强依赖变成弱依赖，甚至无依赖，应用不再关心微服务集群，这使得自身变得非常的灵活和轻量，还有个好处就是框架升级不会导致应用升级，上面说的服务间的调用、配置、全局锁、负载均衡、路由都由 Mesh 实现，这种灵活性给项目管理带来了很大的福音，也很优雅。

组成 Service Mesh 的两个核心组件为控制组件和数据组件，数据组件以代理模式跟应用一起部署，如部署在同一个 Pod 里，应用所有的调用都通过这个代理，即应用访问 http://127.0.0.1/service 请求，“代理”接管对内对外的所有应用请求，并实现控制、安全、流量、服务发现注册等功能。数据层的实现主要是 Linked 和 Envoy ，所有代理进程劫持所有应用网络，整体组成一个格子网络，并统一由上层控制器进行调度，这个像格子的网络就被称做 mesh 了。

2017年5月24日，Google 和 IBM 高调宣布基于 Service Mesh 思想的服务化框架产品 Istio 0.1 发布，出身名门的 Istio 火了，给出身草根的 Linked 承重一击，Linked 瞬间陷入了黑暗，Istio 只做控制层，数据层收编了 Envoy ，上层控制层提供 Pilot、Mixer、Istio-Auth 三大组件：

1. Envoy: 与应用部署在一起，提供服务间请求高效转发，并提供扩展接口以实现不同的转发策略，同时上报流量监控数据，提供 HTTP、gRPC、TCP 转发能力。
* Mixer：提供监控数据管理、路由、负载均衡、路由、调用追踪等流量管理，是控制器的核心，并提供后端对接平台，如k8s、Mesos等。
* Pilot（飞行员）: Mixer 的执行模块，负责对 Envoy 进行运行时配置。
* Istio-Auth：提供服务间 TLS 安全通信、角色鉴权、用户认证等 AAA 管理。

![img](/images/istio_arch.svg)

下面介绍下 eBPF

#### eBPF 介绍

Kernel 3.18 中 eBPF(Extended Berkeley Packet Filter)提供了一种在网络栈的钩子节点处运行用户代码的能力，框架具备高扩展性、可编程和动态运行时的能力，eBPF 使用 llvm 把用户 C 语言代码编译成 eBPF 可执行文件在内核态执行，这为上层应用提供了网络安全、流量控制等可能。可编写 eBPF 代码有一定的限制性，也能理解，毕竟运行在内核态的代码是要求非常苛刻的，为了不破坏内核的稳定性，eBPF 提供了叫 verifier 的机制对用户代码进行巡检，确保用户代码符合内核要求，并同时能在短时间执行完毕。

BPF 提供几个重要的编程组件有下：

1. Helper Func：提供一些从内核中读写数据流的函数集。
* Maps：内核中存储 KV 的 Map 集。
* Object Pinning
* Tail Calls：用户一个 BPF 应用调用另一个 BPF 应用。
* BPF to BPF Calls：最新内核新增了 BPF 应用间直接调用能力。
* JIT：即时翻译执行代码能力
* Hardening：
* Offloads：允许用户代码下沉到网卡中执行

##### BPF 程序的限制

1. 在 Kernel 4.16 和 LLVM 6.0 之前不支持普通函数调用，所有调用必须为内联函数
* 最大只能执行 4096 个 BPF 指令
* 不支持共享库调用（使用 bfp/lib 定义的库）
* 不允许全局变量，但可以使用 `BPF_MAP_TYPE_PERCPU_ARRAY` 作为全局 map 存储状态信息，并可以在多个 BPF 程序间共享数据
* 不允许使用字符串常量和数组
* 限制性的使用循环，BPF verifier 验证程序会检测代码是否有循环，使用 `#pragma unroll` 和 `BPF_MAP_TYPE_PERCPU_ARRAY` 最大只能支持 32 次迭代
* 栈空间限制大小 512 bytes

知道 BPF 是从 Cilium 中了解到的，下面介绍下 Cilium

#### Cilium

Cilium 是一个强大的以 eBPF 为基础的网络框架，能做到 L347 层的安全策略、流量控制，并且性能高、灵活性强，主要解决微服务的使用场景，官方解释如下：

`Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes.
The development of modern datacenter applications has shifted to a service-oriented architecture often referred to as microservices, wherein a large application is split into small independent services that communicate with each other via APIs using lightweight protocols like HTTP. `

Cilium 就定位为微服务解决网络问题。

`Microservices applications tend to be highly dynamic, with individual containers getting started or destroyed as the application scales out / in to adapt to load changes and during rolling updates that are deployed as part of continuous delivery.
This shift toward highly dynamic microservices presents both a challenge and an opportunity in terms of securing connectivity between microservices. `

要解决微服务架构下的网络问题的挑战和机会在于要适应微服务快速变化能力，微服务扩容和升级会非常频繁，从而倒逼网络和安全策略也需要适应这种快速变化。

`Traditional Linux network security approaches (e.g., iptables) filter on IP address and TCP/UDP ports, but IP addresses frequently churn in dynamic microservices environments. The highly volatile life cycle of containers causes these approaches to struggle to scale side by side with the application as load balancing tables and access control lists carrying hundreds of thousands of rules that need to be updated with a continuously growing frequency.`

而使用传统的以 IP Base 的策略模型已经不再适应当今以微服务为模型的架构了，微服务的底层网络变化会非常灵活和迅速，如二层的网络 IP 地址可以迅速变化，传统安全策略模型在微服务架构下会产生成百上千条规则，并且这种规则会被频繁变更。

`By leveraging Linux BPF, Cilium retains the ability to transparently insert security visibility + enforcement, but does so in a way that is based on service / pod / container identity (in contrast to IP address identification in traditional systems) and can filter on application-layer (e.g. HTTP). As a result, Cilium not only makes it simple to apply security policies in a highly dynamic environment by decoupling security from addressing, but can also provide stronger security isolation by operating at the HTTP-layer in addition to providing traditional Layer 3 and Layer 4 segmentation.`

因此 Cilium 利用 BPF 的能力，以 service / pod / container 为基础进行动态网络和安全策略管理，不仅仅把安全策略与变化的环境解耦，同时还提供了上层 HTTP 层的能力。


Cilium 架构如下：

![img](/images/cilium.png)

上层对接控制编排面，下层转换成 BPF 的程序注入到内核网络栈执行。

Cilium 已实现的功能如下：

1. 支持 HTTP 协议，支持 method、path、host、headers 匹配的策略
* 支持 Kafka 的协议，支持 Role、topic 匹配的策略管理
* 支持 IP/CIDR、Label、Service、Entities Base 的策略管理
* 负载均衡
* 监控和故障定位，支持对接 Prometheus 监控平台

#### 一个简单的 BPF 例子

1. 开发环境准备

  The following applies to Ubuntu 17.04 or later:

  ```sh
  $ sudo apt-get install -y make gcc libssl-dev bc libelf-dev libcap-dev \
  clang gcc-multilib llvm libncurses5-dev git pkg-config libmnl bison flex \
  graphviz
  ```

* 编码代码 tc-example.c 实现对进出流量传输字节进行计数

  ```c
  #include <linux/bpf.h>
  #include <linux/pkt_cls.h>
  #include <stdint.h>
  #include <iproute2/bpf_elf.h>

  #ifndef __section
  # define __section(NAME)                  \
     __attribute__((section(NAME), used))
  #endif

  #ifndef __inline
  # define __inline                         \
     inline __attribute__((always_inline))
  #endif

  #ifndef lock_xadd
  # define lock_xadd(ptr, val)              \
     ((void)__sync_fetch_and_add(ptr, val))
  #endif

  #ifndef BPF_FUNC
  # define BPF_FUNC(NAME, ...)              \
     (*NAME)(__VA_ARGS__) = (void *)BPF_FUNC_##NAME
  #endif

  static void *BPF_FUNC(map_lookup_elem, void *map, const void *key);

  struct bpf_elf_map acc_map __section("maps") = {
      .type           = BPF_MAP_TYPE_ARRAY,
      .size_key       = sizeof(uint32_t),
      .size_value     = sizeof(uint32_t),
      .pinning        = PIN_GLOBAL_NS,
      .max_elem       = 2,
  };

  static __inline int account_data(struct __sk_buff *skb, uint32_t dir)
  {
      uint32_t *bytes;

      bytes = map_lookup_elem(&acc_map, &dir);
      if (bytes)
              lock_xadd(bytes, skb->len);

      return TC_ACT_OK;
  }

  __section("ingress")
  int tc_ingress(struct __sk_buff *skb)
  {
      return account_data(skb, 0);
  }

  __section("egress")
  int tc_egress(struct __sk_buff *skb)
  {
      return account_data(skb, 1);
  }

  char __license[] __section("license") = "GPL";

  /**
   *	struct sk_buff - socket buffer, it's the primary struct for network.
   *    See  https://elixir.bootlin.com/linux/latest/source/include/linux/skbuff.h)
   *	@next: Next buffer in list
   *	@prev: Previous buffer in list
   *	@tstamp: Time we arrived/left
   *	@rbnode: RB tree node, alternative to next/prev for netem/tcp
   *	@sk: Socket we are owned by
   *	@dev: Device we arrived on/are leaving by
   *	@cb: Control buffer. Free for use by every layer. Put private vars here
   *	@_skb_refdst: destination entry (with norefcount bit)
   *	@sp: the security path, used for xfrm
   *	@len: Length of actual data
   *	@data_len: Data length
   *	@mac_len: Length of link layer header
   *	@hdr_len: writable header length of cloned skb
   *	@csum: Checksum (must include start/offset pair)
   *	@csum_start: Offset from skb->head where checksumming should start
   *	@csum_offset: Offset from csum_start where checksum should be stored
   *	@priority: Packet queueing priority
   *	@ignore_df: allow local fragmentation
   *	@cloned: Head may be cloned (check refcnt to be sure)
   *	@ip_summed: Driver fed us an IP checksum
   *	@nohdr: Payload reference only, must not modify header
   *	@pkt_type: Packet class
   *	@fclone: skbuff clone status
   *	@ipvs_property: skbuff is owned by ipvs
   *	@tc_skip_classify: do not classify packet. set by IFB device
   *	@tc_at_ingress: used within tc_classify to distinguish in/egress
   *	@tc_redirected: packet was redirected by a tc action
   *	@tc_from_ingress: if tc_redirected, tc_at_ingress at time of redirect
   *	@peeked: this packet has been seen already, so stats have been
   *		done for it, don't do them again
   *	@nf_trace: netfilter packet trace flag
   *	@protocol: Packet protocol from driver
   *	@destructor: Destruct function
   *	@tcp_tsorted_anchor: list structure for TCP (tp->tsorted_sent_queue)
   *	@_nfct: Associated connection, if any (with nfctinfo bits)
   *	@nf_bridge: Saved data about a bridged frame - see br_netfilter.c
   *	@skb_iif: ifindex of device we arrived on
   *	@tc_index: Traffic control index
   *	@hash: the packet hash
   *	@queue_mapping: Queue mapping for multiqueue devices
   *	@xmit_more: More SKBs are pending for this queue
   *	@ndisc_nodetype: router type (from link layer)
   *	@ooo_okay: allow the mapping of a socket to a queue to be changed
   *	@l4_hash: indicate hash is a canonical 4-tuple hash over transport
   *		ports.
   *	@sw_hash: indicates hash was computed in software stack
   *	@wifi_acked_valid: wifi_acked was set
   *	@wifi_acked: whether frame was acked on wifi or not
   *	@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS
   *	@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL
   *	@dst_pending_confirm: need to confirm neighbour
    *	@napi_id: id of the NAPI struct this skb came from
   *	@secmark: security marking
   *	@mark: Generic packet mark
   *	@vlan_proto: vlan encapsulation protocol
   *	@vlan_tci: vlan tag control information
   *	@inner_protocol: Protocol (encapsulation)
   *	@inner_transport_header: Inner transport layer header (encapsulation)
   *	@inner_network_header: Network layer header (encapsulation)
   *	@inner_mac_header: Link layer header (encapsulation)
   *	@transport_header: Transport layer header
   *	@network_header: Network layer header
   *	@mac_header: Link layer header
   *	@tail: Tail pointer
   *	@end: End pointer
   *	@head: Head of buffer
   *	@data: Data head pointer
   *	@truesize: Buffer size
   *	@users: User count - see {datagram,tcp}.c
   */

  ```

* 编译成 BPF 可执行程序

  ```sh
  $ clang -O2 -Wall -target bpf -c tc-example.c -o tc-example.o
  ```

* 加载执行程序到网卡

  ```sh
  # tc qdisc add dev em1 clsact
  # tc filter add dev em1 ingress bpf da obj tc-example.o sec ingress
  # tc filter add dev em1 egress bpf da obj tc-example.o sec egress

  # tc filter show dev em1 ingress
  filter protocol all pref 49152 bpf
  filter protocol all pref 49152 bpf handle 0x1 tc-example.o:[ingress] direct-action id 1 tag c5f7825e5dac396f

  # tc filter show dev em1 egress
  filter protocol all pref 49152 bpf
  filter protocol all pref 49152 bpf handle 0x1 tc-example.o:[egress] direct-action id 2 tag b2fd5adc0f262714

  # mount | grep bpf
  sysfs on /sys/fs/bpf type sysfs (rw,nosuid,nodev,noexec,relatime,seclabel)
  bpf on /sys/fs/bpf type bpf (rw,relatime,mode=0700)

  # tree /sys/fs/bpf/
  /sys/fs/bpf/
  +-- ip -> /sys/fs/bpf/tc/
  +-- tc
  |   +-- globals
  |       +-- acc_map
  +-- xdp -> /sys/fs/bpf/tc/

  4 directories, 1 file
  ```
